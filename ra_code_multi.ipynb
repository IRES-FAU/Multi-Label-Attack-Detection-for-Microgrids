{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit"
  },
  "interpreter": {
   "hash": "2822d8322ce8e2a57bcaf0add4d5b4c3accd408277cc0e3f16d84566ccb8ead1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def get_func_length(x_train, func):\n",
    "    if func == min:\n",
    "        func_length = np.inf\n",
    "    else:\n",
    "        func_length = 0\n",
    "\n",
    "    n = x_train.shape[0]\n",
    "    for i in range(n):\n",
    "        func_length = func(func_length, x_train[i].shape[1])\n",
    "\n",
    "    return func_length\n",
    "\n",
    "\n",
    "def transform_to_same_length(x, n_var, max_length):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # the new set in ucr form np array\n",
    "    ucr_x = np.zeros((n, max_length, n_var), dtype=np.float64)\n",
    "\n",
    "    # loop through each time series\n",
    "    for i in range(n):\n",
    "        mts = x[i]\n",
    "        curr_length = mts.shape[1]\n",
    "        idx = np.array(range(curr_length))\n",
    "        idx_new = np.linspace(0, idx.max(), max_length)\n",
    "        for j in range(n_var):\n",
    "            ts = mts[j]\n",
    "            # linear interpolation\n",
    "            f = interp1d(idx, ts, kind='cubic')\n",
    "            new_ts = f(idx_new)\n",
    "            ucr_x[i, :, j] = new_ts\n",
    "\n",
    "    return ucr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})"
   ]
  },
  {
   "source": [
    "from scipy.io import loadmat\n",
    "# REPLACE WITH YOUR DIRECTORY\n",
    "out_dir = 'C:\\\\Users\\\\'\n",
    "a = loadmat('xy1200.mat')\n",
    "a = a['ra']\n",
    "a = a[0, 0]\n",
    "\n",
    "dt = a.dtype.names\n",
    "dt = list(dt)\n",
    "\n",
    "for i in range(len(dt)):\n",
    "    if dt[i] == 'samples':\n",
    "        x_train = a[i].reshape(max(a[i].shape))\n",
    "    elif dt[i] == 'labels':\n",
    "        y_train = a[i]\n",
    "\n",
    "n_var = x_train[0].shape[0]\n",
    "\n",
    "max_length = get_func_length(x_train, func=max)\n",
    "min_length = get_func_length(x_train, func=min)\n",
    "\n",
    "print('effect_ra ', 'max', max_length, ' min', min_length)\n",
    "\n",
    "x_train = transform_to_same_length(x_train, n_var, max_length)\n",
    "\n",
    "np.save(out_dir + 'x.npy', x_train)\n",
    "np.save(out_dir + 'y.npy', y_train)\n",
    "\n",
    "print('Done')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"x.npy\")\n",
    "target = np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(data, target, test_size=0.2)\n",
    "print('dataset: X_train:', x_train.shape, 'Y_train:', y_train.shape, 'X_test:', x_test.shape, 'Y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout, Flatten\n",
    "from keras.layers import Input, Dense, LSTM, concatenate, Activation, Add, AveragePooling1D\n",
    "from keras.layers import MaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "num_class = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "#inception model\n",
    "def _inception_module(input_tensor, stride=1, activation='linear', use_bottleneck=True, bottleneck_size=32,\n",
    "                      kernel_size=40, nb_filters=32):\n",
    "    if use_bottleneck and int(input_tensor.shape[-1]) > bottleneck_size:\n",
    "        input_inception = Conv1D(filters=bottleneck_size, kernel_size=1,\n",
    "                                 padding='same', activation=activation, use_bias=False)(input_tensor)\n",
    "    else:\n",
    "        input_inception = input_tensor\n",
    "\n",
    "    kernel_size_s = [kernel_size // (2 ** i) for i in range(3)]\n",
    "    conv_list = []\n",
    "    for i in range(len(kernel_size_s)):\n",
    "        conv_list.append(Conv1D(filters=nb_filters, kernel_size=kernel_size_s[i], strides=stride, padding='same',\n",
    "                                activation=activation, use_bias=False)(input_inception))\n",
    "    max_pool_1 = MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "    conv_6 = Conv1D(filters=nb_filters, kernel_size=1, padding='same',\n",
    "                    activation=activation, use_bias=False)(max_pool_1)\n",
    "    conv_list.append(conv_6)\n",
    "    x = Concatenate(axis=2)(conv_list)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation='swish')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _shortcut_layer(input_tensor, out_tensor):\n",
    "    shortcut_y = Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1, padding='same', use_bias=False)(input_tensor)\n",
    "    shortcut_y = BatchNormalization()(shortcut_y)\n",
    "    x = Add()([shortcut_y, out_tensor])\n",
    "    x = Activation('swish')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def generate_inception(INPUT_SHAPE, NB_CLASS, use_residual=True, depth=6):\n",
    "    input_layer = Input(INPUT_SHAPE)\n",
    "    x = input_layer\n",
    "    input_res = input_layer\n",
    "    for d in range(depth):\n",
    "        x = _inception_module(x)\n",
    "        if use_residual and d % 3 == 2:\n",
    "            x = _shortcut_layer(input_res, x)\n",
    "            input_res = x\n",
    "    gap_layer = GlobalAveragePooling1D()(x)\n",
    "    output_layer = Dense(NB_CLASS, activation='sigmoid')(gap_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "inception_model = generate_inception(input_shape, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet Model\n",
    "def generate_resnet(INPUT_SHAPE, NB_CLASS):\n",
    "    n_feature_maps = 32\n",
    "    input_layer = Input(INPUT_SHAPE)\n",
    "    # BLOCK 1\n",
    "    conv_x = Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('swish')(conv_x)\n",
    "    conv_y = Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('swish')(conv_y)\n",
    "    conv_z = Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut_y = BatchNormalization()(shortcut_y)\n",
    "    output_block_1 = Add()([shortcut_y, conv_z])\n",
    "    output_block_1 = Activation('swish')(output_block_1)\n",
    "    # BLOCK 2\n",
    "    conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('swish')(conv_x)\n",
    "    conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('swish')(conv_y)\n",
    "    conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut_y = BatchNormalization()(shortcut_y)\n",
    "    output_block_2 = Add()([shortcut_y, conv_z])\n",
    "    output_block_2 = Activation('swish')(output_block_2)\n",
    "    # BLOCK 3\n",
    "    conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('swish')(conv_x)\n",
    "    conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('swish')(conv_y)\n",
    "    conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "    # no need to expand channels because they are equal\n",
    "    shortcut_y = BatchNormalization()(output_block_2)\n",
    "    output_block_3 = Add()([shortcut_y, conv_z])\n",
    "    output_block_3 = Activation('swish')(output_block_3)\n",
    "    # FINAL\n",
    "    gap_layer = GlobalAveragePooling1D()(output_block_3)\n",
    "    output_layer = Dense(NB_CLASS, activation='sigmoid')(gap_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "resnet_model = generate_resnet(input_shape, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best threshold calculator for changing nonbinary output labels to binary\n",
    "\n",
    "from sklearn import metrics\n",
    "import argparse\n",
    "\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "def threshold_calibration_matthews_corrcoef(LABEL, SCORE):\n",
    "    Pred = SCORE\n",
    "    Truth = LABEL\n",
    "    threshold = np.arange(0.1, 0.9, 0.1)\n",
    "    acc = []\n",
    "    accuracies = []\n",
    "    best_threshold = np.zeros(Pred.shape[1])\n",
    "    for i in range(Pred.shape[1]):\n",
    "        y_prob = np.array(Pred[:, i])\n",
    "        for j in threshold:\n",
    "            y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "            acc.append(matthews_corrcoef(Truth[:, i], y_pred))\n",
    "        acc = np.array(acc)\n",
    "        index = np.where(acc == acc.max())\n",
    "        accuracies.append(acc.max())\n",
    "        best_threshold[i] = threshold[index[0][0]]\n",
    "        acc = []\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "#START Inception\n",
    "batch_size = 15\n",
    "num_epoch = 100\n",
    "learning_rate = .001\n",
    "mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, epsilon=1e-4, mode='min')\n",
    "stop_early = EarlyStopping(monitor='val_accuracy', patience=40, mode='max', restore_best_weights=True)\n",
    "optm = Adam(lr=learning_rate)"
   ]
  },
  {
   "source": [
    "seq = inception_model\n",
    "seq.compile(optimizer=optm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = seq.fit(x_train, y_train,\n",
    "                  batch_size=mini_batch_size,\n",
    "                  epochs=num_epoch,\n",
    "                  shuffle=True,\n",
    "                  verbose=1,\n",
    "                  validation_split=0.0,\n",
    "                  validation_data=(x_train, y_train),\n",
    "                  callbacks=[reduce_lr_loss])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, multilabel_confusion_matrix, confusion_matrix\n",
    "y_pred = inception_model.predict(x_test)\n",
    "best_threshold = threshold_calibration_matthews_corrcoef(LABEL=y_train, SCORE=inception_model.predict(x_train))\n",
    "y_pred_binary = np.array(\n",
    "    [[1 if y_pred[i, j] >= best_threshold[j] else 0 for j in range(y_test.shape[1])] for i in range(len(y_test))])\n",
    "\n",
    "for i in range (0,20):\n",
    "    print(y_pred_binary[i])  \n",
    "    print(y_test[i]) \n",
    "    plt.pyplot.plot(x_test[i,:,0:11])\n",
    "    plt.pyplot.show() \n",
    "    print()\n",
    "\n",
    "\n",
    "print('F1_score(Micro):', f1_score(y_test, y_pred_binary, average='micro'))\n",
    "print('F1_score(Macro):', f1_score(y_test, y_pred_binary, average='macro'))\n",
    "print('presicion Score(Micro):', precision_score(y_test, y_pred_binary, average='micro'))\n",
    "print('presicion Score(Macro):', precision_score(y_test, y_pred_binary, average='macro'))\n",
    "print('recall Score(Micro):', recall_score(y_test, y_pred_binary, average='micro'))\n",
    "print('recall Score(Macro):', recall_score(y_test, y_pred_binary, average='macro'))\n",
    "print('Acc_score:', accuracy_score(y_test, y_pred_binary))\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_binary))\n",
    "print('Acc_score Normal:', accuracy_score(y_test[:,0], y_pred_binary[:,0]))\n",
    "print('Acc_score Load Change:', accuracy_score(y_test[:,1], y_pred_binary[:,1]))\n",
    "print('Acc_score Voltage Attack:', accuracy_score(y_test[:,2], y_pred_binary[:,2]))\n",
    "print('Acc_score Power Attack:', accuracy_score(y_test[:,3], y_pred_binary[:,3]))\n",
    "\n",
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(4):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_binary[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred_binary.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "lw = 2"
   ]
  },
  {
   "source": [
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "n_classes = 4\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='Micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'pink'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "      if i == 0:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Normal (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "      if i == 1:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Load Change (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "      if i == 2:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Voltage Attack (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "      if i == 3:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Power Attack (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Inception with Swish ROC by Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START ResNet\n",
    "batch_size = 15\n",
    "num_epoch = 100\n",
    "learning_rate = .001\n",
    "mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, epsilon=1e-4, mode='min')\n",
    "stop_early = EarlyStopping(monitor='val_accuracy', patience=35, mode='max', restore_best_weights=True)\n",
    "optm = Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = resnet_model\n",
    "seq.compile(optimizer=optm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = seq.fit(x_train, y_train,\n",
    "                  batch_size=mini_batch_size,\n",
    "                  epochs=num_epoch,\n",
    "                  shuffle=True,\n",
    "                  verbose=1,\n",
    "                  validation_split=0.0,\n",
    "                  validation_data=(x_train, y_train),\n",
    "                  callbacks=[reduce_lr_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, multilabel_confusion_matrix\n",
    "y_pred = resnet_model.predict(x_test)\n",
    "best_threshold = threshold_calibration_matthews_corrcoef(LABEL=y_train, SCORE=resnet_model.predict(x_train))\n",
    "y_pred_binary = np.array(\n",
    "    [[1 if y_pred[i, j] >= best_threshold[j] else 0 for j in range(y_test.shape[1])] for i in range(len(y_test))])\n",
    "\n",
    "# for i in range(20):\n",
    "#     print(y_pred_binary[i])  \n",
    "#     print(y_test[i]) \n",
    "#     plt.pyplot.plot(x_test[i,:,0:11])\n",
    "#     plt.pyplot.show() \n",
    "#     print()\n",
    "\n",
    "print('F1_score(Micro):', f1_score(y_test, y_pred_binary, average='micro'))\n",
    "print('F1_score(Macro):', f1_score(y_test, y_pred_binary, average='macro'))\n",
    "print('presicion Score(Micro):', precision_score(y_test, y_pred_binary, average='micro'))\n",
    "print('presicion Score(Macro):', precision_score(y_test, y_pred_binary, average='macro'))\n",
    "print('recall Score(Micro):', recall_score(y_test, y_pred_binary, average='micro'))\n",
    "print('recall Score(Macro):', recall_score(y_test, y_pred_binary, average='macro'))\n",
    "print('Acc_score:', accuracy_score(y_test, y_pred_binary))\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_binary))\n",
    "print('Acc_score Normal:', accuracy_score(y_test[:,0], y_pred_binary[:,0]))\n",
    "print('Acc_score Load Change:', accuracy_score(y_test[:,1], y_pred_binary[:,1]))\n",
    "print('Acc_score Voltage Attack:', accuracy_score(y_test[:,2], y_pred_binary[:,2]))\n",
    "print('Acc_score Power Attack:', accuracy_score(y_test[:,3], y_pred_binary[:,3]))\n",
    "\n",
    "\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(4):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_binary[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred_binary.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "lw = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "n_classes = 4\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='Micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'pink'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "      if i == 0:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Normal (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "      if i == 1:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Load Change (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "      if i == 2:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Voltage Attack (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "      if i == 3:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Power Attack (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ResNet with Swish ROC by Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ]
}
